{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOKviWU5N_nV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
        "print(OPENAI_API_KEY)\n",
        "assert OPENAI_API_KEY is not None, \"Please set the OPENAI_API_KEY environment variable\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Database of Research Papers from ArXiV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dbsABSgfANta",
        "outputId": "db8c5a58-e5b9-40de-a8d2-7b76ab438999"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "\n",
        "# makes a directory to store the raw papers\n",
        "if os.path.exists('./raw_documents'):\n",
        "    shutil.rmtree('./raw_documents')\n",
        "os.makedirs('./raw_documents')\n",
        "\n",
        "# TODO: Add the arXiv IDs of the papers you want to download e.g. \"2005.14419\"\n",
        "paper_ids = []\n",
        "client = arxiv.Client()\n",
        "\n",
        "for paper_id in paper_ids:\n",
        "    paper = next(client.results(arxiv.Search(id_list=[paper_id])))\n",
        "    paper.download_pdf(dirpath=\"./raw_documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parse PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOhOFnQL7M9w"
      },
      "outputs": [],
      "source": [
        "from autorag.parser import Parser\n",
        "\n",
        "# makes a directory to store the parsed papers\n",
        "if os.path.exists('./parse_project_dir'):\n",
        "    shutil.rmtree('./parse_project_dir')\n",
        "os.makedirs('./parse_project_dir')\n",
        "\n",
        "# TODO: Complete the parse.yaml file\n",
        "parser = Parser(data_path_glob=\"./raw_documents/*.pdf\", project_dir=\"./parse_project_dir\")\n",
        "parser.start_parsing(\"./parse.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9uGUweTGleP"
      },
      "source": [
        "After the parser run is finished, you can see the result at the `parse_project_dir` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "yCA5i83VVVWY",
        "outputId": "b31028b5-e969-4a75-f02b-b8b5a72bce1d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pdfminer_raw_df = pd.read_parquet(\"./parse_project_dir/parsed_result.parquet\")\n",
        "pdfminer_raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR7imgQtVkjV"
      },
      "source": [
        "In the raw dataframe, you can find out the four columns.\n",
        "\n",
        "- texts : The parsed result. All parsed result from the original documents.\n",
        "- path : The path of the original file\n",
        "- page : The page of the document. If -1, it means whole document.\n",
        "- last_modified_datetime : When the document last modified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Fz7cW2LvW0"
      },
      "source": [
        "# Chunking\n",
        "\n",
        "Chunking is the stage that makes whole documents to little pieces. This is important because embedding model or other retrieval methods is not optimized for the too long documents. It is great to make little passages to increase retrieval performance.\n",
        "\n",
        "You can also use multiple Chunk modules at once. In this case, you need to use one corpus to create QA, and then map the rest of the corpus to QA Data. If the chunk method is different, the retrieval_gt will be different, so we need to remap it to the QA dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSKlVeM7Npz_"
      },
      "outputs": [],
      "source": [
        "from autorag.chunker import Chunker\n",
        "\n",
        "# makes a directory to store the chunked papers\n",
        "if os.path.exists(\"./chunk_project_dir\"):\n",
        "    shutil.rmtree(\"./chunk_project_dir\")\n",
        "os.makedirs(\"./chunk_project_dir\")\n",
        "\n",
        "# TODO: Complete the chunk.yaml\n",
        "chunker = Chunker.from_parquet(parsed_data_path=\"./parse_project_dir/parsed_result.parquet\", project_dir=\"./chunk_project_dir\")\n",
        "chunker.start_chunking(\"./chunk.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_K5-ui3Pfox"
      },
      "source": [
        "After the chunker run is finished, you can see the result at the `chunk_project_dir` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ZTgcc7ywN5yA",
        "outputId": "d6a52572-9d3d-436c-c451-54a83d512965"
      },
      "outputs": [],
      "source": [
        "corpus_df = pd.read_parquet(\"./chunk_project_dir/0.parquet\")\n",
        "print(f\"Split the papers into {len(corpus_df)} chunks\")\n",
        "for idx, row in corpus_df.iterrows():\n",
        "    print(f\"Chunk {idx}: {row['contents']}\")\n",
        "    print(\"#\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFs-4UpkQ34H"
      },
      "source": [
        "# QA generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qFqGPg2QAO4"
      },
      "outputs": [],
      "source": [
        "from autorag.data.qa.schema import Raw, Corpus\n",
        "\n",
        "raw_df = pd.read_parquet(\"./chunk_project_dir/0.parquet\")\n",
        "raw_instance = Raw(raw_df)\n",
        "\n",
        "corpus_df = pd.read_parquet(\"./chunk_project_dir/0.parquet\")\n",
        "corpus_instance = Corpus(corpus_df, raw_instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVjpCbcnRhvT"
      },
      "source": [
        "Now, let's use LLM to generate questions. These will be used to select the best RAG pipeline for our documents. That's what AutoRAG does in the end :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwi2sse8RSdK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from autorag.data.qa.filter.dontknow import dontknow_filter_rule_based\n",
        "from autorag.data.qa.generation_gt.llama_index_gen_gt import make_basic_gen_gt, make_concise_gen_gt\n",
        "from autorag.data.qa.query.llama_gen_query import factoid_query_gen\n",
        "from autorag.data.qa.sample import random_single_hop\n",
        "\n",
        "# Configurations\n",
        "# TODO: Pick an OpenAI model\n",
        "OPENAI_MODEL = \"\"\n",
        "# TODO: Set the number of QA pairs to generate (more will use more credits but can yield more precise evaluations of the RAG pipeline)\n",
        "NUM_QA = -1\n",
        "SAVE_PATH = \"./papers_data\"\n",
        "\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    shutil.rmtree(SAVE_PATH)\n",
        "os.makedirs(SAVE_PATH)\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenAI(model=OPENAI_MODEL)\n",
        "\n",
        "# Generate initial QA dataset\n",
        "initial_qa = (\n",
        "    corpus_instance.sample(random_single_hop, n=NUM_QA)\n",
        "    .map(lambda df: df.reset_index(drop=True))\n",
        "    .make_retrieval_gt_contents()\n",
        "    .batch_apply(factoid_query_gen, llm=llm)\n",
        "    .batch_apply(make_basic_gen_gt, llm=llm)\n",
        "    .batch_apply(make_concise_gen_gt, llm=llm)\n",
        "    .filter(dontknow_filter_rule_based, lang=\"en\")\n",
        ")\n",
        "\n",
        "# Save the initial QA dataset\n",
        "initial_qa.to_parquet(f\"{SAVE_PATH}/papers_qa.parquet\", f\"{SAVE_PATH}/papers_corpus.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_parquet = pd.read_parquet(f\"{SAVE_PATH}/papers_qa.parquet\")\n",
        "train_qa, test_qa = train_test_split(qa_parquet, test_size=0.2, random_state=42)\n",
        "train_qa.to_parquet(f\"{SAVE_PATH}/train_qa.parquet\")\n",
        "test_qa.to_parquet(f\"{SAVE_PATH}/test_qa.parquet\")\n",
        "print(\"Train and test QA parquet files saved successfully\")\n",
        "print(f\"Train QA shape: {train_qa.shape}\")\n",
        "print(f\"Test QA shape: {test_qa.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Mwbdxm9Sobq",
        "outputId": "b8438398-36b9-4029-bc98-43206685c0c0"
      },
      "outputs": [],
      "source": [
        "qa_train = pd.read_parquet(f\"{SAVE_PATH}/train_qa.parquet\")\n",
        "NUM_EXAMPLES_TO_PRINT = min(50, len(qa_train))\n",
        "\n",
        "for i in range(NUM_EXAMPLES_TO_PRINT):\n",
        "    print(f\"Query {i+1}:\")\n",
        "    print(\"Q:\", qa_train.iloc[i][\"query\"])\n",
        "    print(\"A:\", qa_train.iloc[i][\"generation_gt\"][0])\n",
        "    print(\"#\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "autorag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
